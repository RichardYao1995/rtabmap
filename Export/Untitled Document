#include <iostream>
#include <fstream>
#include <list>
#include <vector>
#include <chrono>
#include <ctime>
#include <climits>

#include <opencv2/core/core.hpp>
#include <opencv2/imgproc/imgproc.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/features2d/features2d.hpp>
#include <opencv2/calib3d/calib3d.hpp>

//#include <g2o/core/base_unary_edge.h>
//#include <g2o/core/block_solver.h>
//#include <g2o/core/optimization_algorithm_levenberg.h>
//#include <g2o/solvers/dense/linear_solver_dense.h>
//#include <g2o/core/robust_kernel.h>
//#include <g2o/core/sparse_optimizer.h>
//#include <g2o/types/sba/types_six_dof_expmap.h>

#include "rtabmap/core/DBDriver.h"
#include "rtabmap/core/Rtabmap.h"
#include "rtabmap/core/util3d.h"
#include "rtabmap/core/util3d_filtering.h"
#include "rtabmap/core/util3d_transforms.h"
#include "rtabmap/core/util3d_surface.h"
#include "rtabmap/core/util2d.h"
#include "rtabmap/utilite/UMath.h"
#include "rtabmap/utilite/UTimer.h"
#include "rtabmap/utilite/UFile.h"

#include "ceres/ceres.h"
#include "ceres/rotation.h"


using namespace std;
//using namespace g2o;
using namespace rtabmap;
using namespace cv;

/********************************************
 * 本节演示了RGBD上的半稠密直接法 
 ********************************************/

// 一次测量的值，包括一个世界坐标系下三维点与一个灰度值
struct Measurement
{
    Measurement ( Eigen::Vector3d p, float g ) : pos_world ( p ), grayscale ( g ) {}
    Eigen::Vector3d pos_world;
    float grayscale;
};

inline Eigen::Vector3d project2Dto3D ( int x, int y, int d, float fx, float fy, float cx, float cy, float scale )
{
    float zz = float ( d ) /scale;
    float xx = zz* ( x-cx ) /fx;
    float yy = zz* ( y-cy ) /fy;
    return Eigen::Vector3d ( xx, yy, zz );
}

inline Eigen::Vector2d project3Dto2D ( float x, float y, float z, float fx, float fy, float cx, float cy )
{
    float u = fx*x/z+cx;
    float v = fy*y/z+cy;
    return Eigen::Vector2d ( u,v );
}
bool poseEstimationDirect(const std::vector< Measurement >& measurements, cv::Mat* gray, Eigen::Matrix3f& K, Eigen::Isometry3d& Tcw);

class SparseBA : public ceres::SizedCostFunction<1,6>
{
public:
  cv::Mat * gray_;
  double cx_, cy_;
  double fx_, fy_;

  double pixelValue_;
  double X_, Y_, Z_;

SparseBA(cv::Mat *gray, double cx, double cy, double fx, double fy, double X, double Y, double Z, double pixelValue)
{
  gray_ = gray;
  cx_ = cx;
  cy_ = cy;
  fx_ = fx;
  fy_ = fy;
  X_ = X;
  Y_ = Y;
  Z_ = Z;
  pixelValue_ = pixelValue;
}

virtual bool Evaluate (double const *const *pose, double *residual, double **jacobians) const{
  //存储p的坐标
  double p[3];
  p[0] = X_;
  p[1] = Y_;
  p[2] = Z_;

  //存储新的p'的坐标
  double newP[3];
  double R[3];
  R[0] = pose[0][0];
  R[1] = pose[0][1];
  R[2] = pose[0][2];
  ceres::AngleAxisRotatePoint(R, p, newP);

  newP[0] += pose[0][3];
  newP[1] += pose[0][4];
  newP[2] += pose[0][5];

  //新的p‘点投影到像素坐标系
  double ux = fx_ * newP[0] / newP[2] + cx_;
  double uy = fy_ * newP[1] / newP[2] + cy_;

  residual[0] = getPixelValue(ux, uy) - pixelValue_;

  if (jacobians)
  {
    double invz = 1.0 / newP[2];
    double invz_2 = invz * invz;

    //公式8.15
    Eigen::Matrix<double, 2, 6> jacobian_uv_ksai;
    jacobian_uv_ksai(0,0) = -newP[0] * newP[1] * invz_2 * fx_;
    jacobian_uv_ksai(0,1) = (1 + (newP[0] * newP[0] * invz_2)) * fx_;
    jacobian_uv_ksai(0,2) = -newP[1] * invz * fx_;
    jacobian_uv_ksai(0,3) = invz * fx_;
    jacobian_uv_ksai(0,4) = 0;
    jacobian_uv_ksai(0,5) = -newP[0] * invz_2 * fx_;

    jacobian_uv_ksai(1,0) = -(1 + newP[1] * newP[1] * invz_2) * fy_;
    jacobian_uv_ksai(1,1) = newP[0] * newP[1] * invz_2 * fy_;
    jacobian_uv_ksai(1,2) = newP[0] * invz * fy_;
    jacobian_uv_ksai(1,3) = 0;
    jacobian_uv_ksai(1,4) = invz * fy_;
    jacobian_uv_ksai(1,5) = -newP[1] * invz_2 * fy_;

    //像素梯度
    Eigen::Matrix<double, 1, 2> jacobian_pixel_uv;
    jacobian_pixel_uv(0,0) = (getPixelValue(ux+1, uy) - getPixelValue(ux-1, uy))/2;
    jacobian_pixel_uv(0,1) = (getPixelValue(ux, uy+1) - getPixelValue(ux, uy-1))/2;

    //公式8.16
    Eigen::Matrix<double, 1, 6> jacobian = jacobian_pixel_uv * jacobian_uv_ksai;

    jacobians[0][0] = jacobian(0);
    jacobians[0][1] = jacobian(1);
    jacobians[0][2] = jacobian(2);
    jacobians[0][3] = jacobian(3);
    jacobians[0][4] = jacobian(4);
    jacobians[0][5] = jacobian(5);
  }

  return true;

}

double getPixelValue (double x, double y) const
{
  uchar* data = & gray_->data[int(y) * gray_->step + int(x)];
  double xx = x - floor(x);
  double yy = y - floor(y);
  return double (
    (1 - xx) * (1 - yy) * data[0] + xx * (1 - yy) * data[1] + (1 - xx) * yy * data[gray_->step] + xx * yy * data[gray_->step + 1]
  );
}
};

int main ()
{       
    ULogger::setType(ULogger::kTypeConsole);
    ULogger::setLevel(ULogger::kError);

    bool isOptimate = 0;
    bool isGlobal   = 1;

    std::string dbPath = "/home/uisee/Documents/RTAB-Map/190715-091852.db";

    // Get parameters
    ParametersMap parameters;
    DBDriver * driver = DBDriver::create();
    if(driver->openConnection(dbPath))
    {
        parameters = driver->getLastParameters();
        driver->closeConnection(false);
    }
    else
    {
        UERROR("Cannot open database %s!", dbPath.c_str());
    }
    delete driver;

    // Get the global optimized map
    Rtabmap rtabmap;
    rtabmap.init(parameters, dbPath);

    std::map<int, Signature> nodes;
    std::map<int, Transform> optimizedPoses;
    std::multimap<int, Link> links;
    rtabmap.get3DMap(nodes, optimizedPoses, links, isOptimate, isGlobal);
    std::map<int, Transform>::iterator iter=optimizedPoses.begin();
    for(size_t i = 0;i < optimizedPoses.size() / 4;i++)
        iter++;

    Signature node = nodes.find(iter->first)->second;
    node.sensorData().uncompressData();
    StereoCameraModel model = node.sensorData().stereoCameraModel();

    cv::Mat color, disparity, gray, right;
    right = node.sensorData().rightRaw();
    color = node.sensorData().imageRaw();
    cv::cvtColor(color, gray, CV_BGR2GRAY);
    disparity = util2d::disparityFromStereoImages(gray, right);
    vector<Measurement> measurements;
    // 相机内参
    float cx = 640;
    float cy = 320;
    float fx = 762.72;
    float fy = 762.72;
    Eigen::Matrix3f K;
    K<<fx,0.f,cx,0.f,fy,cy,0.f,0.f,1.0f;

    Eigen::Isometry3d Tcw = Eigen::Isometry3d::Identity();

    cv::Mat prev_color;
    // 我们以第一个图像为参考，对后续图像和参考图像做直接法
    for ( int x=10; x<gray.cols-10; x++ )
        for ( int y=10; y<gray.rows-10; y++ )
        {
            Eigen::Vector2d delta (
                gray.ptr<uchar>(y)[x+1] - gray.ptr<uchar>(y)[x-1],
                gray.ptr<uchar>(y+1)[x] - gray.ptr<uchar>(y-1)[x]
            );
            if ( delta.norm() < 70 )
                continue;
            float disp = disparity.at<float>(y,x);

            cv::Point3f point = util3d::projectDisparityTo3D(cv::Point2f(x, y), disp, model);
            Eigen::Vector3d p3d((double)point.x, (double)point.y, (double)point.z);
            float grayscale = float ( gray.ptr<uchar> (y) [x] );
            measurements.push_back ( Measurement ( p3d, grayscale ) );
        }
    prev_color = color.clone();
    cout<<"add total "<<measurements.size()<<" measurements."<<endl;

    iter++;

    node = nodes.find(iter->first)->second;
    node.sensorData().uncompressData();

    color = node.sensorData().imageRaw();
    cv::Mat gray1;
    cv::cvtColor(color, gray1, CV_BGR2GRAY);

        // 使用直接法计算相机运动
        chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
        poseEstimationDirect ( measurements, &gray1, K, Tcw );
        chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
        chrono::duration<double> time_used = chrono::duration_cast<chrono::duration<double>> ( t2-t1 );
        cout<<"direct method costs time: "<<time_used.count() <<" seconds."<<endl;
        cout<<"Tcw="<<Tcw.matrix() <<endl;

        // plot the feature points
        cv::Mat img_show ( color.rows*2, color.cols, CV_8UC3 );
        prev_color.copyTo ( img_show ( cv::Rect ( 0,0,color.cols, color.rows ) ) );
        color.copyTo ( img_show ( cv::Rect ( 0,color.rows,color.cols, color.rows ) ) );
        for ( Measurement m:measurements )
        {
            if ( rand() > RAND_MAX/5 )
                continue;
            Eigen::Vector3d p = m.pos_world;
            Eigen::Vector2d pixel_prev = project3Dto2D ( p ( 0,0 ), p ( 1,0 ), p ( 2,0 ), fx, fy, cx, cy );
            Eigen::Vector3d p2 = Tcw*m.pos_world;
            Eigen::Vector2d pixel_now = project3Dto2D ( p2 ( 0,0 ), p2 ( 1,0 ), p2 ( 2,0 ), fx, fy, cx, cy );
            if ( pixel_now(0,0)<0 || pixel_now(0,0)>=color.cols || pixel_now(1,0)<0 || pixel_now(1,0)>=color.rows )
                continue;

            float b = 0;
            float g = 250;
            float r = 0;
            img_show.ptr<uchar>( pixel_prev(1,0) )[int(pixel_prev(0,0))*3] = b;
            img_show.ptr<uchar>( pixel_prev(1,0) )[int(pixel_prev(0,0))*3+1] = g;
            img_show.ptr<uchar>( pixel_prev(1,0) )[int(pixel_prev(0,0))*3+2] = r;

            img_show.ptr<uchar>( pixel_now(1,0)+color.rows )[int(pixel_now(0,0))*3] = b;
            img_show.ptr<uchar>( pixel_now(1,0)+color.rows )[int(pixel_now(0,0))*3+1] = g;
            img_show.ptr<uchar>( pixel_now(1,0)+color.rows )[int(pixel_now(0,0))*3+2] = r;
            cv::circle ( img_show, cv::Point2d ( pixel_prev ( 0,0 ), pixel_prev ( 1,0 ) ), 4, cv::Scalar ( b,g,r ), 2 );
            cv::circle ( img_show, cv::Point2d ( pixel_now ( 0,0 ), pixel_now ( 1,0 ) +color.rows ), 4, cv::Scalar ( b,g,r ), 2 );
        }
        cv::imshow ( "result", img_show );
        cv::waitKey ( 0 );


    return 0;
}

bool poseEstimationDirect(const std::vector< Measurement >& measurements, cv::Mat* gray, Eigen::Matrix3f& K, Eigen::Isometry3d& Tcw)
{
  ceres::Problem problem;
  //定义位姿数组
  double pose[6];
  //用轴角进行优化
  Eigen::AngleAxisd rotationVector(Tcw.rotation());
  pose[0] = rotationVector.angle() * rotationVector.axis()(0);
  pose[1] = rotationVector.angle() * rotationVector.axis()(1);
  pose[2] = rotationVector.angle() * rotationVector.axis()(2);
  pose[3] = Tcw.translation()(0);
  pose[4] = Tcw.translation()(1);
  pose[5] = Tcw.translation()(2);

  //构建Ceres问题
  for (Measurement m:measurements)
  {
    ceres::CostFunction * costFunction = new SparseBA(gray, K(0,2), K(1,2), K(0,0), K(1,1), m.pos_world(0), m.pos_world(1), m.pos_world(2), double(m.grayscale));
    problem.AddResidualBlock(costFunction, NULL, pose);
  }

  ceres::Solver::Options options;
  options.num_threads = 4;
  options.linear_solver_type = ceres::DENSE_QR;
  options.minimizer_progress_to_stdout = true;
  ceres::Solver::Summary summary;
  ceres::Solve(options, &problem, &summary);

  cv::Mat rotateVectorCV = cv::Mat::zeros(3, 1, CV_64FC1);
  rotateVectorCV.at<double>(0) = pose[0];
  rotateVectorCV.at<double>(1) = pose[1];
  rotateVectorCV.at<double>(2) = pose[2];

  cv::Mat RCV;
  cv::Rodrigues(rotateVectorCV, RCV);
  Tcw(0,0) = RCV.at<double>(0,0); Tcw(0,1) = RCV.at<double>(0,1); Tcw(0,2) = RCV.at<double>(0,2);
  Tcw(1,0) = RCV.at<double>(1,0); Tcw(1,1) = RCV.at<double>(1,1); Tcw(1,2) = RCV.at<double>(1,2);
  Tcw(2,0) = RCV.at<double>(2,0); Tcw(2,1) = RCV.at<double>(2,1); Tcw(2,2) = RCV.at<double>(2,2);

  Tcw(0,3) = pose[3];
  Tcw(1,3) = pose[4];
  Tcw(2,3) = pose[5];
}

